<html><head>
<title>CMSC838B Final</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
body,h1 {font-family: "Raleway", Arial, sans-serif}
h1 {letter-spacing: 6px}
.w3-row-padding img {margin-bottom: 12px; margin: auto}
#main {margin: auto}

</style>
<script src="chrome-extension://nngceckbapebfimnlniiiahkandclblb/content/fido2/page-script.js"></script></head>
<body>

<!-- !PAGE CONTENT! -->
<div class="w3-content" style="max-width:1500px">

<!-- Header -->
<header class="w3-panel w3-center w3-opacity" style="padding:20px 16px; padding-bottom: 10px;">
  <h1 class="w3-xlarge">Differentiable Geometry Processing with MeshCNN</h1>
  <h1>CMSC838B Final Report</h1>
  <h6>Sathwik Yanamaddi</h6>
  
  <div class="w3-padding-32">
    <div class="w3-bar w3-border">
      <a href="#" class="w3-bar-item w3-button">Home</a>
      <a href="#" class="w3-bar-item w3-button w3-light-grey">Proposal</a>
      <a href="#" class="w3-bar-item w3-button">Progress</a>
      <a href="#" class="w3-bar-item w3-button w3-hide-small">Final</a>
    </div>
  </div>
</header>

<!-- Photo Grid -->
<div id="main" class="w3-row-padding w3-grayscale" style="margin-bottom:128px; margin-top:10px; padding:5rem; padding-top:0rem; padding-bottom:0rem; text-align: center; font-size: 20px; color: black">
  The availability of 3D shape data significantly increased over the past years due to advances in 3D capturing sensors and 3D modeling tools such as Blender. 
  This naturally leads to a growing interest in the high-level understanding of 3D shapes. Specifically, the part segmentation of 3D shapes gives valuable insights into the characteristics of an object by dividing it into its semantic regions.
  Several deep learning frameworks exist to perform a part segmentation of 3D objects. 
  One popular example is MeshCNN [1], which works directly on the edges of 3D mesh representations. This geometric approach is highly promising for real world appslications, but MeshCNN is limited
  in its functionality due to its bias towards a balanced class distribution & limited input size. Can we rework MeshCNN for tasks involving imbalanced datasets and fine-grained segmentation results?
  <img id="firstImage" src="https://cdn.discordapp.com/attachments/1038128245729677382/1186750014039535768/Screenshot_2023-12-19_at_14.19.07.png" height="360" alt="animationImg"><br>
  
</div>

<div class="w3-row-padding w3-grayscale" style="margin-bottom:128px; margin-top:0px; padding:8rem; padding-top: 0rem;">
  <p>
    <a href="https://docs.google.com/presentation/d/18nOloyzerxXcKcBJCRebpSJAxannnsnv1Y-4HCaQmro/edit#slide=id.g4dfce81f19_0_45"><h5>Link to Slides</h5></a>
    <strong>Link to Final Presentation</strong>
  </p>
  <h4>Motivation &amp; Background</h4>
  We wish to enhance existing audio visualizer apps by 
  having Virtual Reality in mind. We will make use of space and depth in 
  conjunction with beat detection and spectrum analysis data for all kinds of creative possibilities 
  for colors, lights, and shapes around you. 
  
  <br>
  <br>
  <img id="firstImage" src="https://cdn.discordapp.com/attachments/941210710619332658/960609070421798962/unknown.png" height="360" alt="animationImg"><br>
  

  <h4>State-of-art and new challenges</h4>
  Implementing the audio visualization as it syncs with the music. <br>
  Physics of visualizations as it interacts with other objects in the environment <br>
  Creating an immersive environment where you feel like you are in space with visualizers being everywhere where you look <br>

  <h4>Proposed Tasks</h4>
  Short-term: Create a room with audio that syncs up with basic visual representation of the audio components. <br>
  Long-term: Add visual customization in the form of colors, shapes, and audio unmixing. Add more features for quality of life of the application (changing volume, pitch, etc. )


  <h4>Possible ideas that we plan to pursue</h4>
  Audio unmixing
  Sound propagation
  User choice to change type of environments that affects the audio visualization
  Visualizations forming around the room and possibly bouncing around with physics
  User can hear different intensities of the audio spectrum depending on where they stand in the room
  Can change the materials of the room walls to affect sound propagation (open ceiling, concert hall, outdoor concert, etc. ). 

  <h4>Minimum expected accomplishments by April 25th</h4>
  Have basic visualizations appear in the environment that syncs with the music <br>
  Have our “concert” room made with audio able to be played <br>
  Button to start and stop audio <br>

  <h4>Minimum expected accomplishments by mid-May</h4>
  Have a completely immersiveness concert-space-like environment where visuals of varying colors are being formed, flying about, and eventually fading out. 
  <br>
  <a href="https://docs.google.com/presentation/d/1KBAiWTcie7gTj2n8LH6b2Zq73PS-VWj8VVnUkihlWwo/edit?usp=sharing"><h5>Link to Slides</h5></a>
  
  <br>
  <br>
  <h2>Progress Report</h2>
  <video width="320" height="240" controls="">
      <source src="FInalProjectProgressDemo.mp4" type="video/mp4">
  </video>
  <br>
  <br>
  <h4>What has been done so far : </h4>
- Created dome in blender <br>
- Created simple audio visualizer <br>
- Successfully unmixed multiple songs into four streams (drums/bass/vocals/other) <br>
- Created objects to represent these in our audio visualizer <br>
  <br>
  
  <h4>Currently working on : </h4>
- code for having objects represent and react to a certain part of a song <br>
- letting certain parts of a song be louder when you stand closer to them <br>
- adding sound synthesis to the materials of the dome and possibly add different environments <br>
- more visually pleasing features  <br>
  
  <h4> Other goals: </h4>
- Add buttons to mute certain parts of a song so that you can listen to specific parts (ex: ONLY drums or ONLY vocals) 
  
 <h1> References </h1>
 <p>
    [1] Hanocka, R., Hertz, A., Fish, N., Giryes, R., Fleishman, S., Cohen-Or, D., 2019. Meshcnn: a network with an edge. ACM Transactionson Graphics (TOG) 38, 1–12
 </p>
  
</div>

 
  
  
  
  
  
<!-- End Page Content -->
</div>

<!-- Footer -->
<footer class="w3-container w3-padding-64 w3-light-grey w3-center w3-large"> 
  <!-- <i class="fa fa-facebook-official w3-hover-opacity"></i>
  <i class="fa fa-instagram w3-hover-opacity"></i>
  <i class="fa fa-snapchat w3-hover-opacity"></i>
  <i class="fa fa-pinterest-p w3-hover-opacity"></i>
  <i class="fa fa-twitter w3-hover-opacity"></i>
  <i class="fa fa-linkedin w3-hover-opacity"></i> -->
  
</footer>



</body><style>
@media print {
  #simplifyJobsContainer {
    display: none;
  }
}
</style><div id="simplifyJobsContainer" style="position: absolute; top: 0px; left: 0px; width: 0px; height: 0px; overflow: visible; z-index: 2147483647;"><span></span></div><script id="simplifyJobsPageScript" src="chrome-extension://pbanhockgagggenencehbnadejlgchfc/js/pageScript.bundle.js"></script></html>
